---
title: "What makes a popular Reddit post?"
author: "InterstellR"
date: "May 2, 2018"
bibliography: bibliography.bib
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Load packages and data

```{r load-packages, echo=FALSE, message=FALSE}
library(chunked)
library(lubridate)
library(tidytext)
library(tidyverse)
library(scales)
```

```{r load-data, echo=FALSE, message=FALSE}
# This may take a while
posts <- read_csv("../data/reddit.csv")
```

## Introduction

"Reddit is an American social news aggregation, web content rating, and discussion website" (from [Wikipedia](https://en.wikipedia.org/wiki/Reddit)). People who are members of Reddit upload their posts and other people vote and comment on said posts, the posts with the most likes appear more towards the top. They are divided by thematic categories called subreddits, as well as overall categories of top posts, new posts, controversial posts, among others.

TODO: Expand introduction

### Data Set

The data set is comprised of Reddit posts from December 2017, since it was the latest data set we could find and wanted to keep up with the most recent trends. Also,  it will be interesting to explore themes related to Christmas. We retrieved it from the Google BigQuery API. The data set contains `r ncol(posts)` variables and `r nrow(posts)` observations. 

Our dataset is made up of `r ncol(posts)` variables but for this project, we will only be looking at these variables: `subreddit` (categorical, name of the subreddit the post belongs to), `num_comments` (numerical, the number of comments on the post),`score` (numerical, popularity score of post), `ups` (numerical, amount of up votes the post recieved), `downs` (numerical, amount of down votes the post recieved), `title` (categorical, the title of the post), `selftext` (categorical, the text body on the post), `gilded` (numerical, amount of gold reddit donations from other users), `over_18` ( categorical, true or fale if post is only appropriate for users over the age of 18). The rest of the descriptions for the other variables can be found in our data folder. 


### Research Question

Coincidentally, our team is made up of avid Reddit fans. All three of us are constantly checking it for interesting, insightful, and funny posts. Given that Reddit recieves thousands of posts per day and the little time in the day we have to check Reddit, we all usually only check the "popular" feed. So, it got us thinking that we should learn more about the site we use pretty frequently. Specifically, we want to analyze what makes a Reddit post popular.

## Cleaning

The dataset we acquired is mostly clean; however, there is one thing we need to clean up before we began our analysis. The variable `created_utc` gives the date that the post was created as the difference in milliseconds from January 1st, 1970. This quantity is not very easy to read or interpret, so we convert it to a combination of day, hour, and minute. We do not need to store the month and year since these values are the same for all entries in the dataset.

```{r clean}
posts <- mutate(
  posts,
  created_date = as.POSIXct(created_utc, origin = "1970-01-01") + 5 * 60 * 60,
  created_day = day(created_date),
  created_hour = hour(created_date),
  created_minute = minute(created_date)
)
#posts <- mutate(
#  posts,
#  retrieved_date = as.POSIXct(retrieved_on, origin = "1970-01-01") + 5 * 60 * 60,
#  retrieved_day = day(retrieved_date),
#  retrieved_hour = hour(retrieved_date),
#  retrieved_minute = minute(retrieved_date)
#)
```

To check whether our conversion is correct, we can look at the range of the new date variable. We see that the first post in the dataset was on `r min(posts$created_date)` and the latest one was on `r max(posts$created_date)`. Since this range is exactly the month of December 2017, we can be confident that the conversion was successful.

## Exploring Popular Subreddits

As stricty popular-feed Reddit users, we do not know much about the various subreddits yet. Hence, we want to start by finding out what the most popular ones are and gain insights into what the posts in each one are about. As a measure of a subreddit's popularity, we decide to use the cumulative score of all of its posts. The plot below shows the nine subreddits with the highest resulting totals.

```{r active-subreddits}
popular_subs <- posts %>%
  group_by(subreddit) %>%
  summarize(
    num_posts = n(),
    total_score = sum(score)
  ) %>%
  arrange(desc(total_score)) %>%
  head(9)
ggplot(popular_subs, aes(x = reorder(subreddit, total_score), y = total_score / 1e6)) +
  geom_col() +
  labs(
    title = "Most popular subreddits",
    subtitle = "in December 2017",
    x = "",
    y = "Cumulative score of all posts"
  ) +
  scale_y_continuous(
    label = unit_format("M"),
    breaks = seq(0, 25, length = 6),
    expand = c(0, 0)
  ) +
  coord_flip() +
  theme_minimal()
```

We find that the nine most popular subreddits in descending order are `The_Donald`, `funny`, `politics`, `aww`, `pics`, `gaming`, `gifs`, `todayilearned`, and `me_irl`. We may be able to intuit what most of these subreddits are about from their name alone, but we wanted to know more about the content and central topics in each of them. Therefore, we decided to analyze the most frequent terms in each of these subreddits with the help of the `tidytext` package. We want to focus on the text contained in the title of the posts since it is the first thing that is visible to the users. Furthermore, we decided to filter out stop words such as "the" and "a" because they naturally appear very frequently and do not convey much about the content of any one particular subreddit.

```{r tf-analysis}
# count most frequent words for each subreddit
tidy_titles <- posts %>%
  filter(subreddit %in% popular_subs$subreddit) %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words, by = "word") %>%
  count(subreddit, word, sort = TRUE) %>%
  group_by(subreddit) %>%
  top_n(10, wt = n) %>%
  ungroup() %>%
  arrange(subreddit, n) %>%
  mutate(order = row_number())
# reorder subreddit variable by popularity
tidy_titles <- tidy_titles %>%
  left_join(popular_subs, by = "subreddit") %>%
  mutate(subreddit = reorder(subreddit, desc(total_score)))
# plot results
ggplot(tidy_titles, aes(x = order, y = n / 1e3, fill = subreddit)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~subreddit, scales = "free") +
  labs(
    title = "Most frequent words within various subreddits",
    subtitle = "in December 2017",
    x = "",
    y = "Count (thousands)"
  ) +
  scale_x_continuous(
    breaks = tidy_titles$order,
    labels = tidy_titles$word,
    expand = c(0, 0)
  ) +
  scale_y_continuous(
    expand = c(0, 0)
  ) +
  coord_flip() +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank())
```

TODO: Discuss results for this visualization

## Sentiment Analysis

One of the major questions we want to explore is whether positive or negative posts are more popular. Our hypothesis is that negative posts are generally more popular, as psychologists claim that humans have an innate "negativity bias" which draws them toward bad news moreso than good ones [@ito]. To test this belief, we need a way to determine the sentiment polarity of each post, i.e. how positive or negative it is. We will use the `afinn` sentiment lexicon, which provides a list of 2476 words, each associated with a score between -3 and +3. Negative words are given a negative score, and positive words are given a positive score. The higher in magnitude the score is, the more extreme the negativity or positivity of the word. For example, the word "death" has a score of -3 whereas the word "love" has a score of +3. In order to determine the overall sentiment of a post, we will simply add up the sentiments of the words contained in it. Note that due to the size of the dataframe, we cannot fully unnest is because that would not fit into memory. Hence, we process the data chunk wise.

```{r sentiment-scores}
sents <- get_sentiments("afinn")
tidy_posts <- tibble()
posts %>%
  head(100) %>%
  read_chunkwise(chunk_size = 1e4) %>%
  unnest_tokens(word, title) %>%
  inner_join(sents, suffix = c("", "_sent")) %>%
  group_by(id) %>%
  summarize(sentiment = sum(score_sent)) %>%
  write_chunkwise(tidy_posts)
```

## Cats vs. Dogs

TODO: Hypothesis test whether dogs or cats are more popular

## Modeling Popularity

```{r lm-popularityh}
full_model <- lm(score ~ num_comments + ups + downs + gilded + over
                        age + cls_perc_eval + cls_did_eval + 
                         cls_students + cls_level + cls_profs + 
                         cls_credits + bty_avg, data = posts)

TODO: Density plot and summary statistics for popularity
TODO: Build linear and bayesian model and compare results

## Discussion

TODO: Discuss results

## Conclusion

## References
